{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SW0FD0493kL"
   },
   "source": [
    "# dgl mixhop on cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dWpCJM8Rzs_E",
    "outputId": "54e64396-6ba6-4af9-e572-6412cba37ccd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "\n",
    "from dgl.data import CoraGraphDataset, CiteseerGraphDataset, PubmedGraphDataset,PPIDataset\n",
    "from tqdm import trange\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9xSB7ITSaxQ"
   },
   "source": [
    "## mixhop dgl version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "6VYCx_eJGfjU"
   },
   "outputs": [],
   "source": [
    "class MixHopConvDGL(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_dim,\n",
    "                 out_dim,\n",
    "                 p=[0, 1, 2],\n",
    "                 dropout=0,\n",
    "                 activation=None,\n",
    "                 batchnorm=False):\n",
    "        super(MixHopConvDGL, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.p = p\n",
    "        self.activation = activation\n",
    "        self.batchnorm = batchnorm\n",
    "\n",
    "        # define dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # define batch norm layer\n",
    "        if self.batchnorm:\n",
    "            self.bn = nn.BatchNorm1d(out_dim * len(p))\n",
    "        \n",
    "        # define weight dict for each power j\n",
    "        self.weights = nn.ModuleDict({\n",
    "            str(j): nn.Linear(in_dim, out_dim, bias=False) for j in p\n",
    "        })\n",
    "\n",
    "    def forward(self, graph, feats):\n",
    "        with graph.local_scope():\n",
    "            # assume that the graphs are undirected and graph.in_degrees() is the same as graph.out_degrees()\n",
    "            degs = graph.in_degrees().float().clamp(min=1)\n",
    "            norm = torch.pow(degs, -0.5).to(feats.device).unsqueeze(1) #对graph做标准化\n",
    "\n",
    "            max_j = max(self.p) + 1\n",
    "            outputs = []\n",
    "\n",
    "            for j in range(max_j):\n",
    "\n",
    "                if j in self.p:\n",
    "                    output = self.weights[str(j)](feats)\n",
    "                    outputs.append(output)\n",
    "\n",
    "                feats = feats * norm\n",
    "                graph.ndata['h'] = feats\n",
    "\n",
    "                graph.update_all(fn.copy_u('h', 'm'), fn.sum('m', 'h')) # dgl的update all\n",
    "\n",
    "                # 第一个function为message passing function 对应 pyg 中的message function\n",
    "                # 第二个function 为reduce function，即聚合过程，对应pyg中的super().__init__(aggr='add') 的聚合\n",
    "\n",
    "                feats = graph.ndata.pop('h')\n",
    "                feats = feats * norm\n",
    "            \n",
    "            final = torch.cat(outputs, dim=1)\n",
    "            \n",
    "            if self.batchnorm:\n",
    "                final = self.bn(final)\n",
    "            \n",
    "            if self.activation is not None:\n",
    "                final = self.activation(final)\n",
    "            \n",
    "            final = self.dropout(final)\n",
    "\n",
    "            return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Fg-yRwWSztCE"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MixHopDGL(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_dim,\n",
    "                 hid_dim, \n",
    "                 out_dim,\n",
    "                 num_layers=2,\n",
    "                 p=[0, 1, 2],\n",
    "                 input_dropout=0.0,\n",
    "                 layer_dropout=0.0,\n",
    "                 activation=None,\n",
    "                 batchnorm=False):\n",
    "        super(MixHopDGL, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.p = p\n",
    "        self.input_dropout = input_dropout\n",
    "        self.layer_dropout = layer_dropout\n",
    "        self.activation = activation\n",
    "        self.batchnorm = batchnorm\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(self.input_dropout)\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(MixHopConvDGL(self.in_dim,\n",
    "                                      self.hid_dim,\n",
    "                                      p=self.p,\n",
    "                                      dropout=self.input_dropout,\n",
    "                                      activation=self.activation,\n",
    "                                      batchnorm=self.batchnorm))\n",
    "        \n",
    "        # Hidden layers with n - 1 MixHopConv layers\n",
    "        for i in range(self.num_layers - 2):\n",
    "            self.layers.append(MixHopConvDGL(self.hid_dim * len(p),\n",
    "                                          self.hid_dim,\n",
    "                                          p=self.p,\n",
    "                                          dropout=self.layer_dropout,\n",
    "                                          activation=self.activation,\n",
    "                                          batchnorm=self.batchnorm))\n",
    "        \n",
    "        self.fc_layers = nn.Linear(self.hid_dim * len(p), self.out_dim, bias=False)\n",
    "\n",
    "    def forward(self, graph, feats):\n",
    "        feats = self.dropout(feats)\n",
    "        for layer in self.layers:\n",
    "            feats = layer(graph, feats)\n",
    "        \n",
    "        feats = self.fc_layers(feats)\n",
    "\n",
    "        return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Fil2245Sdpw"
   },
   "source": [
    "## mixhop pyg version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "3xQ1jdqMSgBJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, Parameter\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "class MixHopConv(MessagePassing):\n",
    "    def __init__(self,\n",
    "                 in_dim,\n",
    "                 out_dim,\n",
    "                 p=[0, 1, 2],\n",
    "                 dropout=0,\n",
    "                 activation=None,\n",
    "                 batchnorm=False):\n",
    "      \n",
    "        super().__init__(aggr='add')  \n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.p = p\n",
    "        self.activation = activation\n",
    "        self.batchnorm = batchnorm\n",
    "\n",
    "        # define dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # define batch norm layer\n",
    "        if self.batchnorm:\n",
    "            self.bn = nn.BatchNorm1d(out_dim * len(p))\n",
    "        \n",
    "        # define weight dict for each power j\n",
    "        self.weights = nn.ModuleDict({\n",
    "            str(j): nn.Linear(in_dim, out_dim, bias=False) for j in p\n",
    "        })\n",
    "\n",
    "        self.max_j =  max(self.p) + 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        feats = x\n",
    "\n",
    "\n",
    "        for j in range(self.max_j):\n",
    "\n",
    "            if j in self.p:\n",
    "                output = self.weights[str(j)](feats)\n",
    "                outputs.append(output)\n",
    "\n",
    "            \n",
    "            feats = self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "        final = torch.cat(outputs, dim=1)\n",
    "\n",
    "        if self.batchnorm:\n",
    "            final = self.bn(final)\n",
    "        \n",
    "        if self.activation is not None:\n",
    "            final = self.activation(final)\n",
    "        \n",
    "        final = self.dropout(final)\n",
    "\n",
    "        return final\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "\n",
    "        return norm.view(-1, 1) * x_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "YDtTzCRVSiNx"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MixHop(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_dim,\n",
    "                 hid_dim, \n",
    "                 out_dim,\n",
    "                 num_layers=2,\n",
    "                 p=[0, 1, 2],\n",
    "                 input_dropout=0.0,\n",
    "                 layer_dropout=0.0,\n",
    "                 activation=None,\n",
    "                 batchnorm=False):\n",
    "        super(MixHop, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.p = p\n",
    "        self.input_dropout = input_dropout\n",
    "        self.layer_dropout = layer_dropout\n",
    "        self.activation = activation\n",
    "        self.batchnorm = batchnorm\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(self.input_dropout)\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(MixHopConv(self.in_dim,\n",
    "                                      self.hid_dim,\n",
    "                                      p=self.p,\n",
    "                                      dropout=self.input_dropout,\n",
    "                                      activation=self.activation,\n",
    "                                      batchnorm=self.batchnorm))\n",
    "        \n",
    "        # Hidden layers with n - 1 MixHopConv layers\n",
    "        for i in range(self.num_layers - 2):\n",
    "            self.layers.append(MixHopConv(self.hid_dim * len(p),\n",
    "                                          self.hid_dim,\n",
    "                                          p=self.p,\n",
    "                                          dropout=self.layer_dropout,\n",
    "                                          activation=self.activation,\n",
    "                                          batchnorm=self.batchnorm))\n",
    "        \n",
    "        self.fc_layers = nn.Linear(self.hid_dim * len(p), self.out_dim, bias=False)\n",
    "\n",
    "    def forward(self, x,edge_index):\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, edge_index)\n",
    "        \n",
    "        x = self.fc_layers(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saF4F4yjAeFJ"
   },
   "source": [
    "## cora dgl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qt0egfu5ztE0",
    "outputId": "80440f8e-2ff2-469c-c97a-d4acc580d77f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "dataset = CoraGraphDataset()\n",
    "graph = dataset[0]\n",
    "graph = dgl.add_self_loop(graph)\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "zV7MRcA0Awox"
   },
   "outputs": [],
   "source": [
    "n_classes = dataset.num_classes\n",
    "\n",
    "labels = graph.ndata.pop('label').to(device).long()\n",
    "\n",
    "feats = graph.ndata.pop('feat').to(device)\n",
    "n_features = feats.shape[-1]\n",
    "\n",
    "train_mask = graph.ndata.pop('train_mask')\n",
    "val_mask = graph.ndata.pop('val_mask')\n",
    "test_mask = graph.ndata.pop('test_mask')\n",
    "\n",
    "train_idx = torch.nonzero(train_mask, as_tuple=False).squeeze().to(device)\n",
    "val_idx = torch.nonzero(val_mask, as_tuple=False).squeeze().to(device)\n",
    "test_idx = torch.nonzero(test_mask, as_tuple=False).squeeze().to(device)\n",
    "\n",
    "graph = graph.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RBiZr6cRAtMa",
    "outputId": "3426ba13-d15b-4e07-f1e3-a8a64a85c461"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Acc 1.0000 | Train Loss 0.0115 | Val Acc 0.7180 | Val loss 1.0674:   5%|▍         | 49/1000 [00:06<02:00,  7.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stop.\n",
      "Test Acc 0.7500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = MixHopDGL(in_dim=n_features,\n",
    "                hid_dim=100,\n",
    "                out_dim=n_classes,\n",
    "                num_layers=3,\n",
    "                p=[0, 1, 2],\n",
    "                input_dropout=0.6,\n",
    "                layer_dropout=0.0,\n",
    "                activation=torch.tanh,\n",
    "                batchnorm=True)\n",
    "\n",
    "model = model.to(device)\n",
    "best_model = copy.deepcopy(model)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(opt, 40, gamma=0.01)\n",
    "\n",
    "\n",
    "acc = 0\n",
    "no_improvement = 0\n",
    "epochs = trange(1000, desc='Accuracy & Loss')\n",
    "early_stopping = 25\n",
    "\n",
    "for _ in epochs:\n",
    "    # Training using a full graph\n",
    "    model.train()\n",
    "\n",
    "    logits = model(feats, edge_index)\n",
    "\n",
    "    # compute loss\n",
    "    train_loss = loss_fn(logits[train_idx], labels[train_idx])\n",
    "    train_acc = torch.sum(logits[train_idx].argmax(dim=1) == labels[train_idx]).item() / len(train_idx)\n",
    "\n",
    "    # backward\n",
    "    opt.zero_grad()\n",
    "    train_loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    # Validation using a full graph\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        valid_loss = loss_fn(logits[val_idx], labels[val_idx])\n",
    "        valid_acc = torch.sum(logits[val_idx].argmax(dim=1) == labels[val_idx]).item() / len(val_idx)\n",
    "\n",
    "    # Print out performance\n",
    "    epochs.set_description('Train Acc {:.4f} | Train Loss {:.4f} | Val Acc {:.4f} | Val loss {:.4f}'.format(\n",
    "        train_acc, train_loss.item(), valid_acc, valid_loss.item()))\n",
    "    \n",
    "    if valid_acc < acc:\n",
    "        no_improvement += 1\n",
    "        if no_improvement == early_stopping:\n",
    "            print('Early stop.')\n",
    "            break\n",
    "    else:\n",
    "        no_improvement = 0\n",
    "        acc = valid_acc\n",
    "        best_model = copy.deepcopy(model)\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "best_model.eval()\n",
    "logits = best_model(graph, feats)\n",
    "test_acc = torch.sum(logits[test_idx].argmax(dim=1) == labels[test_idx]).item() / len(test_idx)\n",
    "\n",
    "print(\"Test Acc {:.4f}\".format(test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxdQ17UjS4pa"
   },
   "source": [
    "## cora pyg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kFYxv7IpUZxx",
    "outputId": "5745e6a5-b201-4a7a-827e-37896a74b42c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "dataset = CoraGraphDataset()\n",
    "graph = dataset[0]\n",
    "graph = dgl.add_self_loop(graph)\n",
    "device = 'cpu'\n",
    "\n",
    "n_classes = dataset.num_classes\n",
    "\n",
    "labels = graph.ndata.pop('label').to(device).long()\n",
    "\n",
    "feats = graph.ndata.pop('feat').to(device)\n",
    "n_features = feats.shape[-1]\n",
    "\n",
    "train_mask = graph.ndata.pop('train_mask')\n",
    "val_mask = graph.ndata.pop('val_mask')\n",
    "test_mask = graph.ndata.pop('test_mask')\n",
    "\n",
    "train_idx = torch.nonzero(train_mask, as_tuple=False).squeeze().to(device)\n",
    "val_idx = torch.nonzero(val_mask, as_tuple=False).squeeze().to(device)\n",
    "test_idx = torch.nonzero(test_mask, as_tuple=False).squeeze().to(device)\n",
    "\n",
    "graph = graph.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M_QosBUtTKTK",
    "outputId": "08a1abd6-099b-43ac-876f-0d62d3696aa6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    0,    0,  ..., 2705, 2706, 2707],\n",
       "        [ 633, 1862, 2582,  ..., 2705, 2706, 2707]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index = torch.vstack(graph.edges()).contiguous()\n",
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "IrGTPpF8UB0N"
   },
   "outputs": [],
   "source": [
    "model = MixHop(in_dim=n_features,\n",
    "                hid_dim=100,\n",
    "                out_dim=n_classes,\n",
    "                num_layers=3,\n",
    "                p=[0, 1, 2],\n",
    "                input_dropout=0.6,\n",
    "                layer_dropout=0.0,\n",
    "                activation=torch.tanh,\n",
    "                batchnorm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "CKfR_d_5UE-g"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "best_model = copy.deepcopy(model)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(opt, 40, gamma=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tXmGw2BvUFAq",
    "outputId": "376b6f15-9a57-4989-c66d-3d6fadf91053"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Accuracy & Loss:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "no_improvement = 0\n",
    "epochs = trange(1000, desc='Accuracy & Loss')\n",
    "early_stopping = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ucbzcWAOUNkv",
    "outputId": "3e7be1e4-91c2-4db5-a5fe-a48930a6ddcf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Acc 1.0000 | Train Loss 0.0163 | Val Acc 0.6640 | Val loss 1.1821:   8%|▊         | 80/1000 [00:18<03:33,  4.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stop.\n",
      "Test Acc 0.7690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in epochs:\n",
    "    # Training using a full graph\n",
    "    model.train()\n",
    "\n",
    "    logits = model(feats, edge_index)\n",
    "\n",
    "    # compute loss\n",
    "    train_loss = loss_fn(logits[train_idx], labels[train_idx])\n",
    "    train_acc = torch.sum(logits[train_idx].argmax(dim=1) == labels[train_idx]).item() / len(train_idx)\n",
    "\n",
    "    # backward\n",
    "    opt.zero_grad()\n",
    "    train_loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    # Validation using a full graph\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        valid_loss = loss_fn(logits[val_idx], labels[val_idx])\n",
    "        valid_acc = torch.sum(logits[val_idx].argmax(dim=1) == labels[val_idx]).item() / len(val_idx)\n",
    "\n",
    "    # Print out performance\n",
    "    epochs.set_description('Train Acc {:.4f} | Train Loss {:.4f} | Val Acc {:.4f} | Val loss {:.4f}'.format(\n",
    "        train_acc, train_loss.item(), valid_acc, valid_loss.item()))\n",
    "    \n",
    "    if valid_acc < acc:\n",
    "        no_improvement += 1\n",
    "        if no_improvement == early_stopping:\n",
    "            print('Early stop.')\n",
    "            break\n",
    "    else:\n",
    "        no_improvement = 0\n",
    "        acc = valid_acc\n",
    "        best_model = copy.deepcopy(model)\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "best_model.eval()\n",
    "logits = best_model(feats, edge_index)\n",
    "test_acc = torch.sum(logits[test_idx].argmax(dim=1) == labels[test_idx]).item() / len(test_idx)\n",
    "\n",
    "print(\"Test Acc {:.4f}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
