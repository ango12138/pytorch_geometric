from typing import Tuple

import torch
from torch import Tensor, tensor

from torch_geometric.explain import Explanation


def get_groundtruth_metrics(
    explanation: Explanation,
    groundtruth: Explanation,
    threshold: float = 0.0,
) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:
    r"""Compares an explanation with the ground truth explanation. Returns
    basic evaluation metrics - accuracy, recall, precision, f1_score, and
    auroc.

    Args:
        explanation (Explanation): Explanation output generated by an
            explainer.
        groundtruth (Explanation): Groundtruth for explanation.
        threshold (float): threshold value to perform hard thresholding of the
            `explanation` and `groundtruth` masks. (default: :obj:`0.0`)

    Returns:
        :class:`Tuple`: A 5-item tuple containing the values corresponding
            to the calculated accuracy, recall, precision, f1_score and
            auc on the explanation against the groundtruth given the threshold.

    :rtype: (:class:`Tensor`, :class:`Tensor`, :class:`Tensor`,
        :class:`Tensor`, :class:`Tensor`)
    """
    from torchmetrics import AUROC

    ex_masks = [explanation[key] for key in explanation.available_explanations]
    ex_mask_tensor = torch.cat(list(map(lambda x: x.view(-1), ex_masks)))
    gt_masks = [groundtruth[key] for key in groundtruth.available_explanations]
    gt_mask_tensor = torch.cat(list(map(lambda x: x.view(-1), gt_masks)))

    gt_mask_tensor[gt_mask_tensor > threshold] = 1.0
    auroc = AUROC(task="binary")
    auc = auroc(ex_mask_tensor, gt_mask_tensor.bool())
    ex_mask_tensor[ex_mask_tensor > threshold] = 1.0
    correct_preds = gt_mask_tensor == ex_mask_tensor
    incorrect_preds = gt_mask_tensor != ex_mask_tensor

    tp = torch.sum(gt_mask_tensor[correct_preds])
    tn = torch.sum(correct_preds) - tp
    fp = torch.sum(ex_mask_tensor[incorrect_preds])
    fn = torch.sum(gt_mask_tensor[incorrect_preds])

    if (tp + fp) == 0:
        precision = tensor(0.0)
    else:
        precision = tp / (tp + fp)

    if (tp + fn) == 0:
        recall = tensor(0.0)
    else:
        recall = tp / (tp + fn)

    if precision == 0.0 or recall == 0.0:
        f1_score = tensor(0.0)
    else:
        f1_score = 2 * (precision * recall) / (precision + recall)

    accuracy = (tp + tn) / (tp + fp + tn + fn)
    return accuracy, recall, precision, f1_score, auc
